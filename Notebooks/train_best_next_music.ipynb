{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listens = pd.read_csv('listens.csv')\n",
    "df_musics = pd.read_csv('musics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['music', 'user', 'datetime'], dtype='object')\n",
      "Index(['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
      "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
      "       'type', 'id', 'uri', 'track_href', 'analysis_url', 'duration_ms',\n",
      "       'time_signature', 'playlist_id', 'category', 'category_id',\n",
      "       'creation_date', '_rid', '_self', '_etag', '_attachments', '_ts',\n",
      "       'album', 'artists', 'available_markets', 'disc_number', 'explicit',\n",
      "       'external_ids', 'external_urls', 'href', 'is_local', 'name',\n",
      "       'popularity', 'preview_url', 'track_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_listens.columns)\n",
    "print(df_musics.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>music</th>\n",
       "      <th>user</th>\n",
       "      <th>datetime</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>disc_number</th>\n",
       "      <th>explicit</th>\n",
       "      <th>external_ids</th>\n",
       "      <th>external_urls</th>\n",
       "      <th>href</th>\n",
       "      <th>is_local</th>\n",
       "      <th>name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>preview_url</th>\n",
       "      <th>track_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1vv2owN12FAwGDwHcsnphg</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20T15:40:21</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.925</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1vv2owN12FAw...</td>\n",
       "      <td>False</td>\n",
       "      <td>Clap Your Hands</td>\n",
       "      <td>48.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/81169dad3de74349...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>3ZFTkvIE7kyPt6Nu3PEa7V</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20T16:56:26</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.824</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.892</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3ZFTkvIE7kyP...</td>\n",
       "      <td>False</td>\n",
       "      <td>Hips Don't Lie (feat. Wyclef Jean)</td>\n",
       "      <td>86.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/374b492571c9ba59...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>3xqtvSfofDvSMVwwSbuvKi</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20T18:15:05</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.395</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3xqtvSfofDvS...</td>\n",
       "      <td>False</td>\n",
       "      <td>careless (feat. daniel caesar)</td>\n",
       "      <td>55.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/695ab97d0721a195...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0Gew3eyKPiQ9ehxv0UxL4l</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20T18:42:37</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.602</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0630</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0Gew3eyKPiQ9...</td>\n",
       "      <td>False</td>\n",
       "      <td>Only Love</td>\n",
       "      <td>21.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/f0c1c6f492faad3a...</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>50z6C3DC3n8hYrgYtzUZ3i</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20T18:49:10</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.727</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.332</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/50z6C3DC3n8h...</td>\n",
       "      <td>False</td>\n",
       "      <td>Eu Não Sou Seu Lixo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2EC9IJj7g0mN1Q5VrZkiYY</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20T08:29:03</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.686</td>\n",
       "      <td>9</td>\n",
       "      <td>-15.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2EC9IJj7g0mN...</td>\n",
       "      <td>False</td>\n",
       "      <td>Rebel Rebel - 2016 Remaster</td>\n",
       "      <td>74.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/69a9019284eb21c2...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1EKRPz47FeZHIyV4u0j5GF</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20T09:15:18</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.468</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.781</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1EKRPz47FeZH...</td>\n",
       "      <td>False</td>\n",
       "      <td>You're So Cool</td>\n",
       "      <td>53.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/0914611ca21da05e...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>37PPB2oW67yjvArQPYQxRU</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20T10:50:45</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.842</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.285</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/37PPB2oW67yj...</td>\n",
       "      <td>False</td>\n",
       "      <td>Like a Prayer</td>\n",
       "      <td>36.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/d6f995a2eb0f4812...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>14K8hW2G2QR14sN5nl0VG1</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20T11:15:03</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.612</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.293</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>7yHEDfrJNd0zWOfXwydNH0</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20T11:26:02</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.727</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.852</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7yHEDfrJNd0z...</td>\n",
       "      <td>False</td>\n",
       "      <td>Into You</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      music                                  user   \n",
       "368  1vv2owN12FAwGDwHcsnphg  5e3b7511-4336-453e-8199-a4ec14956742  \\\n",
       "459  3ZFTkvIE7kyPt6Nu3PEa7V  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "364  3xqtvSfofDvSMVwwSbuvKi  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "148  0Gew3eyKPiQ9ehxv0UxL4l  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "271  50z6C3DC3n8hYrgYtzUZ3i  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "..                      ...                                   ...   \n",
       "172  2EC9IJj7g0mN1Q5VrZkiYY  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "109  1EKRPz47FeZHIyV4u0j5GF  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "329  37PPB2oW67yjvArQPYQxRU  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "384  14K8hW2G2QR14sN5nl0VG1  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "423  7yHEDfrJNd0zWOfXwydNH0  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "\n",
       "                datetime  danceability  energy  key  loudness  mode   \n",
       "368  2023-04-20T15:40:21         0.699   0.870    0    -4.925     1  \\\n",
       "459  2023-04-20T16:56:26         0.778   0.824   10    -5.892     0   \n",
       "364  2023-04-20T18:15:05         0.706   0.395   11    -6.952     0   \n",
       "148  2023-04-20T18:42:37         0.705   0.501    0    -7.602     1   \n",
       "271  2023-04-20T18:49:10         0.682   0.727    4    -5.332     0   \n",
       "..                   ...           ...     ...  ...       ...   ...   \n",
       "172  2023-05-20T08:29:03         0.635   0.686    9   -15.648     1   \n",
       "109  2023-05-20T09:15:18         0.614   0.468    1    -7.781     1   \n",
       "329  2023-05-20T10:50:45         0.624   0.842    5    -5.285     1   \n",
       "384  2023-05-20T11:15:03         0.524   0.612    6    -8.293     0   \n",
       "423  2023-05-20T11:26:02         0.636   0.727    9    -5.852     1   \n",
       "\n",
       "     speechiness  acousticness  ...  disc_number  explicit  external_ids   \n",
       "368       0.0546      0.000532  ...          1.0     False           NaN  \\\n",
       "459       0.0707      0.284000  ...          1.0     False           NaN   \n",
       "364       0.0588      0.724000  ...          1.0     False           NaN   \n",
       "148       0.0630      0.277000  ...          1.0     False           NaN   \n",
       "271       0.0396      0.141000  ...          1.0     False           NaN   \n",
       "..           ...           ...  ...          ...       ...           ...   \n",
       "172       0.0530      0.209000  ...          1.0     False           NaN   \n",
       "109       0.0295      0.294000  ...          1.0     False           NaN   \n",
       "329       0.0376      0.264000  ...          1.0     False           NaN   \n",
       "384       0.2410      0.563000  ...          NaN       NaN           NaN   \n",
       "423       0.1060      0.016100  ...          1.0     False           NaN   \n",
       "\n",
       "     external_urls                                               href   \n",
       "368            NaN  https://api.spotify.com/v1/tracks/1vv2owN12FAw...  \\\n",
       "459            NaN  https://api.spotify.com/v1/tracks/3ZFTkvIE7kyP...   \n",
       "364            NaN  https://api.spotify.com/v1/tracks/3xqtvSfofDvS...   \n",
       "148            NaN  https://api.spotify.com/v1/tracks/0Gew3eyKPiQ9...   \n",
       "271            NaN  https://api.spotify.com/v1/tracks/50z6C3DC3n8h...   \n",
       "..             ...                                                ...   \n",
       "172            NaN  https://api.spotify.com/v1/tracks/2EC9IJj7g0mN...   \n",
       "109            NaN  https://api.spotify.com/v1/tracks/1EKRPz47FeZH...   \n",
       "329            NaN  https://api.spotify.com/v1/tracks/37PPB2oW67yj...   \n",
       "384            NaN                                                NaN   \n",
       "423            NaN  https://api.spotify.com/v1/tracks/7yHEDfrJNd0z...   \n",
       "\n",
       "    is_local                                name popularity   \n",
       "368    False                     Clap Your Hands       48.0  \\\n",
       "459    False  Hips Don't Lie (feat. Wyclef Jean)       86.0   \n",
       "364    False      careless (feat. daniel caesar)       55.0   \n",
       "148    False                           Only Love       21.0   \n",
       "271    False                 Eu Não Sou Seu Lixo        0.0   \n",
       "..       ...                                 ...        ...   \n",
       "172    False         Rebel Rebel - 2016 Remaster       74.0   \n",
       "109    False                      You're So Cool       53.0   \n",
       "329    False                       Like a Prayer       36.0   \n",
       "384      NaN                                 NaN        NaN   \n",
       "423    False                            Into You        0.0   \n",
       "\n",
       "                                           preview_url  track_number  \n",
       "368  https://p.scdn.co/mp3-preview/81169dad3de74349...           2.0  \n",
       "459  https://p.scdn.co/mp3-preview/374b492571c9ba59...           3.0  \n",
       "364  https://p.scdn.co/mp3-preview/695ab97d0721a195...          12.0  \n",
       "148  https://p.scdn.co/mp3-preview/f0c1c6f492faad3a...          11.0  \n",
       "271                                                NaN           3.0  \n",
       "..                                                 ...           ...  \n",
       "172  https://p.scdn.co/mp3-preview/69a9019284eb21c2...           6.0  \n",
       "109  https://p.scdn.co/mp3-preview/0914611ca21da05e...           3.0  \n",
       "329  https://p.scdn.co/mp3-preview/d6f995a2eb0f4812...           9.0  \n",
       "384                                                NaN           NaN  \n",
       "423                                                NaN           4.0  \n",
       "\n",
       "[500 rows x 43 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= df_listens.merge(df_musics, how='inner', left_on='music', right_on='id').sort_values( 'datetime')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>music</th>\n",
       "      <th>user</th>\n",
       "      <th>datetime</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>explicit</th>\n",
       "      <th>external_ids</th>\n",
       "      <th>external_urls</th>\n",
       "      <th>href</th>\n",
       "      <th>is_local</th>\n",
       "      <th>name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>preview_url</th>\n",
       "      <th>track_number</th>\n",
       "      <th>time_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1vv2owN12FAwGDwHcsnphg</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20 15:40:21</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.925</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1vv2owN12FAw...</td>\n",
       "      <td>False</td>\n",
       "      <td>Clap Your Hands</td>\n",
       "      <td>48.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/81169dad3de74349...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>3ZFTkvIE7kyPt6Nu3PEa7V</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20 16:56:26</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.824</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.892</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3ZFTkvIE7kyP...</td>\n",
       "      <td>False</td>\n",
       "      <td>Hips Don't Lie (feat. Wyclef Jean)</td>\n",
       "      <td>86.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/374b492571c9ba59...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4565.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>3xqtvSfofDvSMVwwSbuvKi</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20 18:15:05</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.395</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3xqtvSfofDvS...</td>\n",
       "      <td>False</td>\n",
       "      <td>careless (feat. daniel caesar)</td>\n",
       "      <td>55.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/695ab97d0721a195...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0Gew3eyKPiQ9ehxv0UxL4l</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20 18:42:37</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.602</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0630</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0Gew3eyKPiQ9...</td>\n",
       "      <td>False</td>\n",
       "      <td>Only Love</td>\n",
       "      <td>21.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/f0c1c6f492faad3a...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>50z6C3DC3n8hYrgYtzUZ3i</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-04-20 18:49:10</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.727</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.332</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/50z6C3DC3n8h...</td>\n",
       "      <td>False</td>\n",
       "      <td>Eu Não Sou Seu Lixo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2EC9IJj7g0mN1Q5VrZkiYY</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20 08:29:03</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.686</td>\n",
       "      <td>9</td>\n",
       "      <td>-15.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2EC9IJj7g0mN...</td>\n",
       "      <td>False</td>\n",
       "      <td>Rebel Rebel - 2016 Remaster</td>\n",
       "      <td>74.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/69a9019284eb21c2...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4039.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1EKRPz47FeZHIyV4u0j5GF</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20 09:15:18</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.468</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.781</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1EKRPz47FeZH...</td>\n",
       "      <td>False</td>\n",
       "      <td>You're So Cool</td>\n",
       "      <td>53.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/0914611ca21da05e...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>37PPB2oW67yjvArQPYQxRU</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20 10:50:45</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.842</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.285</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/37PPB2oW67yj...</td>\n",
       "      <td>False</td>\n",
       "      <td>Like a Prayer</td>\n",
       "      <td>36.0</td>\n",
       "      <td>https://p.scdn.co/mp3-preview/d6f995a2eb0f4812...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>14K8hW2G2QR14sN5nl0VG1</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20 11:15:03</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.612</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.293</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>7yHEDfrJNd0zWOfXwydNH0</td>\n",
       "      <td>5e3b7511-4336-453e-8199-a4ec14956742</td>\n",
       "      <td>2023-05-20 11:26:02</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.727</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.852</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7yHEDfrJNd0z...</td>\n",
       "      <td>False</td>\n",
       "      <td>Into You</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>659.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      music                                  user   \n",
       "368  1vv2owN12FAwGDwHcsnphg  5e3b7511-4336-453e-8199-a4ec14956742  \\\n",
       "459  3ZFTkvIE7kyPt6Nu3PEa7V  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "364  3xqtvSfofDvSMVwwSbuvKi  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "148  0Gew3eyKPiQ9ehxv0UxL4l  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "271  50z6C3DC3n8hYrgYtzUZ3i  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "..                      ...                                   ...   \n",
       "172  2EC9IJj7g0mN1Q5VrZkiYY  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "109  1EKRPz47FeZHIyV4u0j5GF  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "329  37PPB2oW67yjvArQPYQxRU  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "384  14K8hW2G2QR14sN5nl0VG1  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "423  7yHEDfrJNd0zWOfXwydNH0  5e3b7511-4336-453e-8199-a4ec14956742   \n",
       "\n",
       "               datetime  danceability  energy  key  loudness mode   \n",
       "368 2023-04-20 15:40:21         0.699   0.870    0    -4.925    1  \\\n",
       "459 2023-04-20 16:56:26         0.778   0.824   10    -5.892    0   \n",
       "364 2023-04-20 18:15:05         0.706   0.395   11    -6.952    0   \n",
       "148 2023-04-20 18:42:37         0.705   0.501    0    -7.602    1   \n",
       "271 2023-04-20 18:49:10         0.682   0.727    4    -5.332    0   \n",
       "..                  ...           ...     ...  ...       ...  ...   \n",
       "172 2023-05-20 08:29:03         0.635   0.686    9   -15.648    1   \n",
       "109 2023-05-20 09:15:18         0.614   0.468    1    -7.781    1   \n",
       "329 2023-05-20 10:50:45         0.624   0.842    5    -5.285    1   \n",
       "384 2023-05-20 11:15:03         0.524   0.612    6    -8.293    0   \n",
       "423 2023-05-20 11:26:02         0.636   0.727    9    -5.852    1   \n",
       "\n",
       "     speechiness  acousticness  ...  explicit  external_ids  external_urls   \n",
       "368       0.0546      0.000532  ...     False           NaN            NaN  \\\n",
       "459       0.0707      0.284000  ...     False           NaN            NaN   \n",
       "364       0.0588      0.724000  ...     False           NaN            NaN   \n",
       "148       0.0630      0.277000  ...     False           NaN            NaN   \n",
       "271       0.0396      0.141000  ...     False           NaN            NaN   \n",
       "..           ...           ...  ...       ...           ...            ...   \n",
       "172       0.0530      0.209000  ...     False           NaN            NaN   \n",
       "109       0.0295      0.294000  ...     False           NaN            NaN   \n",
       "329       0.0376      0.264000  ...     False           NaN            NaN   \n",
       "384       0.2410      0.563000  ...       NaN           NaN            NaN   \n",
       "423       0.1060      0.016100  ...     False           NaN            NaN   \n",
       "\n",
       "                                                  href is_local   \n",
       "368  https://api.spotify.com/v1/tracks/1vv2owN12FAw...    False  \\\n",
       "459  https://api.spotify.com/v1/tracks/3ZFTkvIE7kyP...    False   \n",
       "364  https://api.spotify.com/v1/tracks/3xqtvSfofDvS...    False   \n",
       "148  https://api.spotify.com/v1/tracks/0Gew3eyKPiQ9...    False   \n",
       "271  https://api.spotify.com/v1/tracks/50z6C3DC3n8h...    False   \n",
       "..                                                 ...      ...   \n",
       "172  https://api.spotify.com/v1/tracks/2EC9IJj7g0mN...    False   \n",
       "109  https://api.spotify.com/v1/tracks/1EKRPz47FeZH...    False   \n",
       "329  https://api.spotify.com/v1/tracks/37PPB2oW67yj...    False   \n",
       "384                                                NaN      NaN   \n",
       "423  https://api.spotify.com/v1/tracks/7yHEDfrJNd0z...    False   \n",
       "\n",
       "                                   name popularity   \n",
       "368                     Clap Your Hands       48.0  \\\n",
       "459  Hips Don't Lie (feat. Wyclef Jean)       86.0   \n",
       "364      careless (feat. daniel caesar)       55.0   \n",
       "148                           Only Love       21.0   \n",
       "271                 Eu Não Sou Seu Lixo        0.0   \n",
       "..                                  ...        ...   \n",
       "172         Rebel Rebel - 2016 Remaster       74.0   \n",
       "109                      You're So Cool       53.0   \n",
       "329                       Like a Prayer       36.0   \n",
       "384                                 NaN        NaN   \n",
       "423                            Into You        0.0   \n",
       "\n",
       "                                           preview_url track_number  time_gap  \n",
       "368  https://p.scdn.co/mp3-preview/81169dad3de74349...          2.0       NaN  \n",
       "459  https://p.scdn.co/mp3-preview/374b492571c9ba59...          3.0    4565.0  \n",
       "364  https://p.scdn.co/mp3-preview/695ab97d0721a195...         12.0    4719.0  \n",
       "148  https://p.scdn.co/mp3-preview/f0c1c6f492faad3a...         11.0    1652.0  \n",
       "271                                                NaN          3.0     393.0  \n",
       "..                                                 ...          ...       ...  \n",
       "172  https://p.scdn.co/mp3-preview/69a9019284eb21c2...          6.0    4039.0  \n",
       "109  https://p.scdn.co/mp3-preview/0914611ca21da05e...          3.0    2775.0  \n",
       "329  https://p.scdn.co/mp3-preview/d6f995a2eb0f4812...          9.0    5727.0  \n",
       "384                                                NaN          NaN    1458.0  \n",
       "423                                                NaN          4.0     659.0  \n",
       "\n",
       "[500 rows x 44 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['mode', 'time_signature', 'explicit',  'disc_number']\n",
    "float_columns = ['danceability', 'energy', 'loudness',  'speechiness', 'acousticness', 'instrumentalness',\n",
    "       'liveness', 'valence' , 'tempo','duration_ms', 'popularity']\n",
    "\n",
    "\n",
    "df[categorical_columns] =df[categorical_columns].apply(lambda x: x.astype('category'))\n",
    "df[float_columns] =df[float_columns].apply(lambda x: x.astype('float64'))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_gap']  =   df.groupby([ 'user'])['datetime'].diff().dt.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
      "C:\\Users\\netoc\\AppData\\Local\\Temp\\ipykernel_26864\\4009135163.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# separa apenas as colunas interessantes\n",
    "model_columns = float_columns+categorical_columns+['time_gap']\n",
    "train_df = df[model_columns+['user']]\n",
    "\n",
    "dummies= 15\n",
    "#faz a dummificação\n",
    "for d in range (dummies):\n",
    "    for column in model_columns:\n",
    "        train_df[f'{column}_{d}'] = train_df.groupby('user')[column].shift(d)\n",
    "\n",
    "#remove as colunas de modelo\n",
    "train_df= train_df.drop(columns = model_columns+['user']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns\n",
    "\n",
    "y= train_df[[c+'_0' for c in  model_columns]]\n",
    "X = train_df[[c for c in  train_df.columns if c not in y.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators for danceability: 200\n",
      "Mean Squared Error for danceability: 0.005744644991666672\n",
      "Best number of estimators for energy: 10\n",
      "Mean Squared Error for energy: 0.014923956962962972\n",
      "Best number of estimators for loudness: 10\n",
      "Mean Squared Error for loudness: 4.232015408148148\n",
      "Best number of estimators for speechiness: 200\n",
      "Mean Squared Error for speechiness: 0.0011311572566666743\n",
      "Best number of estimators for acousticness: 200\n",
      "Mean Squared Error for acousticness: 0.012951826798111084\n",
      "Best number of estimators for instrumentalness: 200\n",
      "Mean Squared Error for instrumentalness: 0.0022817852750344272\n",
      "Best number of estimators for liveness: 200\n",
      "Mean Squared Error for liveness: 0.003337387328361129\n",
      "Best number of estimators for valence: 500\n",
      "Mean Squared Error for valence: 0.007879951780900774\n",
      "Best number of estimators for tempo: 200\n",
      "Mean Squared Error for tempo: 143.21121055075363\n",
      "Best number of estimators for duration_ms: 50\n",
      "Mean Squared Error for duration_ms: 835753027.5232295\n",
      "Best number of estimators for popularity: 500\n",
      "Mean Squared Error for popularity: 116.9550891851852\n",
      "Best number of estimators for mode: 50\n",
      "Accuracy  for mode: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators for time_signature: 10\n",
      "Accuracy  for time_signature: 1.0\n",
      "Best number of estimators for explicit: 50\n",
      "Accuracy  for explicit: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators for disc_number: 10\n",
      "Accuracy  for disc_number: 0.9629629629629629\n",
      "Best number of estimators for time_gap: 100\n",
      "Mean Squared Error for time_gap: 1801812.0967481479\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Assuming you have two DataFrames: X (features) and y (target)\n",
    "# X should contain the input features, and y should contain the target labels\n",
    "\n",
    "\n",
    "\n",
    "# Define the parameter grid for tuning the number of estimators\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200, 500]}\n",
    "\n",
    "# passa em cada um dos parametros da musica\n",
    "for column in model_columns:\n",
    "\n",
    "    y= train_df[column+'_0']\n",
    "    X = train_df[[c for c in  train_df.columns if '_0' not in c]]\n",
    "\n",
    "    if column in categorical_columns:\n",
    "        rf = RandomForestClassifier()\n",
    "    else:\n",
    "        rf = RandomForestRegressor()\n",
    "\n",
    "\n",
    "    # Perform a grid search with cross-validation to find the best number of estimators\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Get the best number of estimators\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    print(f\"Best number of estimators for {column}:\", best_n_estimators)\n",
    "\n",
    "    # Train the Random Forest Classifier with the best number of estimators\n",
    "    if column in categorical_columns:\n",
    "        best_rf = RandomForestClassifier(n_estimators=best_n_estimators)\n",
    "    else:\n",
    "        best_rf = RandomForestRegressor(n_estimators=best_n_estimators)\n",
    "    best_rf.fit(X, y)\n",
    "\n",
    "    # Make predictions with the trained model\n",
    "    y_pred = best_rf.predict(X)\n",
    "\n",
    "    if column in categorical_columns:\n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Accuracy  for {column}:\", accuracy)\n",
    "\n",
    "    else:\n",
    "        # Calculate mean squared error\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        print(f\"Mean Squared Error for {column}:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_estimators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Início Random Forest\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Criando o modelo de Random Forest\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators \u001b[39m=\u001b[39m n_estimators)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Treinando o modelo\u001b[39;00m\n\u001b[0;32m     14\u001b[0m rf\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_estimators' is not defined"
     ]
    }
   ],
   "source": [
    "# Separa teste e treino\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "n_estimators = \n",
    "# Início Random Forest\n",
    "# Criando o modelo de Random Forest\n",
    "if column in categorical_columns:\n",
    "    rf = RandomForestClassifier(n_estimators = n_estimators)\n",
    "else:\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "# Treinando o modelo\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões com o modelo treinado\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "precision = precision_score(y_test, y_pred, average='micro') * 100\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred) * 100\n",
    "error_rate = 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "danceability_0       0\n",
       "time_signature_0     0\n",
       "mode_0               0\n",
       "tempo_0              0\n",
       "valence_0            0\n",
       "                    ..\n",
       "popularity_13       78\n",
       "disc_number_13      78\n",
       "popularity_14       79\n",
       "explicit_14         79\n",
       "disc_number_14      79\n",
       "Length: 240, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# separa as colunas por tipo\n",
    "categorical_columns = ['mode', 'time_signature', 'explicit',  'disc_number']\n",
    "float_columns = ['danceability', 'energy', 'loudness',  'speechiness', 'acousticness', 'instrumentalness',\n",
    "       'liveness', 'valence' , 'tempo','duration_ms', 'popularity']\n",
    "model_columns = float_columns+categorical_columns+['time_gap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"danceability\"; \"energy\"; \"loudness\"; \"speechiness\"; \"acousticness\"; \"instrumentalness\"; \"liveness\"; \"valence\"; \"tempo\"; \"duration_ms\"; \"popularity\"; \"mode\"; \"time_signature\"; \"explicit\"; \"disc_number\"; \"time_gap\"\n"
     ]
    }
   ],
   "source": [
    "print('; '.join([f'\"{c}\"' for c in model_columns]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
